gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes ## show the names of the top 10 genes
rownames(data) <- paste("gene", 1:15, sep="")
pca <- prcom(t(data))
pca <- prcomp(t(data))
loading_scores <- pca$rotation[,1]
gene_scores <- abs(loading_scores) ## get the magnitudes
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes ## show the names of the top 10 genes
quit()
both/total
candy <- 6
popcorn <- 7
both <- 2
neither <- 3
total <- candy + popcorn - both + neither
candy/total
both/total
total
2/14
1/7
candy/total
only.candy <- 4
only.candy/total
only.popcorn <- 5
only.popcorn/total
5/14
4/14
3/14
0.14 + 0.29 + 0.36 + 0.21
4/14
5/14
3/14
2/7
0.14 / (0.14 + 0.36)
(2/14) / ((2/14) + (5/14))
5/7
5/14
quit()
setwd("~/Google Drive/stat_quests/statquest_boosting_xgboost_part_2_details/")
# install.packages("xgboost")
library(xgboost)
# install.packages("DiagrammeR")
library(DiagrammeR)
## INSTALL or LOAD xgb.Dmatrix
##############################################################
##
## let's create a super simple regression dataset for xgboost...
##
##############################################################
# reg.data <- data.frame(
#   age          =c(12, 13, 45, 47, 73),
#   gender       =c("m","f","m","f","f"),
#   computer.time=c(4,  5,  1,  2,  0))
# reg.data
# reg.data <- data.frame(
#   age          =c(12, 13, 13, 12),
#   gender       =c("m","f", "f", "m"),
#   computer.time=c(4,  5, 5, 4))
# reg.data
reg.data <- data.frame(
age          =c(23, 23, 43, 25),
gender       =c("m","f", "m", "f"),
dosage       =c(10, 20, 25, 35))
reg.data
## NOTE: Gender is a factor. We have to convert it to columns of 0s and 1s
## via "One-hot encoding" for xgboost. This is done to make finding splits
## more efficient.
reg.model <- matrix(model.matrix( ~ -1 +  age + gender + dosage,
data=reg.data), ncol=4)
reg.model
colnames(reg.model) <- c("age", "female", "male", "dosage")
reg.model
# likes.games <- c(1, 1, 0, 0, 0)
# likes.games <- c(10, 9,  0,  2,  0)
# likes.games <- c(10, 9, 9, 10)
# likes.games <- c(10, 9, -10)
# likes.games
drug.effect <- c(-10, 7, 8, -7)
train <- list(data=reg.model, label=drug.effect)
drug.effect
train
## now train an xgboost using regression trees
## data = training data, duh
## label = training labels, duh
## max.depth = [default = ] The maximum number of levels for each tree (alternatively,
##             you can set "max.leaves".)
## eta = [default = 0.3 ] The "step size shrinkage", aka, the learning rate.
##       NOTE: other languages use "learning_rate" as an alias for 'eta'
## nthread = [default = maximum threads avialable ] The number of parallel threads used to
##           run XGBoost I have no idea why you would not just go with the default...
## nrounds = The max number of "boosting iterations". i.e. the maximum number of
##           trees to create.
## objective = [default = reg:squarederror (regression with squared loss)] The
##             learning objective. For classification, try "binary:logistic"
# bst <- xgboost(data = reg.model, label=likes.game, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "reg:squarederror")
# bst <- xgboost(data = train$data, label=train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "reg:logistic")
bst <- xgboost(data = train$data, label=train$label,
max.depth = 2,
gamma=130,
eta = 1,  # step shrinkage, default = 0.3
lambda=0, # l2 regularization, default = 1.
nthread = 2, # number of parallel threads to use, default = max. avail.
nrounds = 2, # maximum number of trees to build (no default)
objective = "reg:squarederror")
predict(bst, newdata=train$data)
## now draw the tree
xgb.plot.tree(model=bst)
library(xgboost)
mydata <- data.frame(y = c(-10, 7, 8, -7), dosage = c(10, 20, 25, 35))
dtrain <- xgb.DMatrix(as.matrix(mydata[, -1]), label = mydata[, "y"])
params <- list(max_depth = 2, objective = "reg:squarederror", gamma = 0, lambda = 0)
mymod <- xgb.train(params, dtrain, nrounds = 1)
xgb.plot.tree(model = mymod)
bst <- xgboost(data = train$data, label=train$label,
max.depth = 2,
gamma=130,
# eta = 1,  # step shrinkage, default = 0.3
lambda=0, # l2 regularization, default = 1.
nthread = 2, # number of parallel threads to use, default = max. avail.
nrounds = 2, # maximum number of trees to build (no default)
objective = "reg:squarederror")
predict(bst, newdata=train$data)
## now draw the tree
xgb.plot.tree(model=bst)
params <- list(max_depth = 2, objective = "reg:squarederror", gamma = 0, lambda = 0, eta=1)
mymod <- xgb.train(params, dtrain, nrounds = 1)
mymod <- xgb.train(params, dtrain, nrounds = 1)
xgb.plot.tree(model = mymod)
0.3 * c(-10.5, 7, -7.5)
quit()
?hclust
quit()
## first let's make a noisy gamma distribution plot...
x <- seq(from=0, to=20, by=0.1)
y.gamma <- dgamma(x, shape=2, scale=2)
y.gamma.scaled <- y.gamma * 100
y.norm <- vector(length=201)
for (i in 1:201) {
y.norm[i] <- rnorm(n=1, mean=y.gamma.scaled[i], sd=2)
}
data <- data.frame(x, y.norm)
plot(data, frame.plot=FALSE, xlab="", ylab="", col="#d95f0e", lwd=1.5)
lo.fit.gamma <- lowess(y.norm ~ x, f=1/5)
plot(data, frame.plot=FALSE, xlab="", ylab="", col="#d95f0e", lwd=1.5)
lines(x, lo.fit.gamma$y, col="black", lwd=3)
lo.fit.gamma$y
plx<-predict(loess(y.norm ~ x, span=1/5, degree=2, family="symmetric", iterations=4), se=T)
plot(data, frame.plot=FALSE, xlab="", ylab="", col="#d95f0e", lwd=1.5)
lines(x, plx$fit, col="black", lwd=3)
str(plx)
plot(data, type="n", frame.plot=FALSE, xlab="", ylab="", col="#d95f0e", lwd=1.5)
polygon(c(x, rev(x)), c(plx$fit + qt(0.975,plx$df)*plx$se, rev(plx$fit - qt(0.975,plx$df)*plx$se)), col="#99999977")
points(data, col="#d95f0e", lwd=1.5)
lines(x, plx$fit, col="black", lwd=3)
quit()
1.3^2
quit()
data <- matrix(c(23,6,117,210), nrow=2, ncol=2, byrow=FALSE)
rownames(data) <- c("Mutated.TRUE", "Mutated.FALSE")
colnames(data) <- c("Cancer.TRUE", "Cancer.FALSE")
data
fisher.test(data)
quit()
0.2^2
0.2^2 * 10
quit()
n = 263
f = 0.57
binom.test(x=(n*f), n=n, p=0.5)
binom.test(x=round(n*f), n=n, p=0.5)
0.67 * 0.19 * (0.10^4)
0.33 * 0.09 * (0.45^4)
9/(17+4)
4/11
0.27 + 0.18 + 0.09 + 0.45
5/11
1/11
2/11
3/11
9 / (17+4)
3/11
quit()
5^5
quit()
5/8
quit()
((0.71) * (0.6))/0.3
8/14
((0.71) * (0.6))/0.57
quit()
35/14
(5/14) * (7/14)
8/14
7/14
(0.5 * 0.18) / 0.57
((5/14) * (7/14))/8/14
((5/14) * (7/14))/(8/14)
(0.5 * 0.31) / 0.57
8/14
(0.71 * 0.5)/0.57
(0.71 * 0.6)/0.57
quit()
4/7
prob <- 4/7
4*log(prob) + 3*log(1-prob)
(-4.8 - (-3.3))/ -4.8
quit()
2*4.5*1.8
1.8^2
(1.8^2)*2
2*1.8*1.8
quit()
0.67 * 0.43 * 0.29
quit()
log(0.5)
log(0.06)
log(0.004)
log(0.5) + log(0.06) + log(0.004) + -115
0.69 + 2.8 + 5.5 + 115
log(0.5)
log(0.000079)
log(0.02)
0.69 + 33.6 + 9.45 + 3.91
quit()
quit()
4/7
quit()
log(2.7)
log2(2.7)
log10(2.7)
quit()
sqrt(3)
x <- seq(from=0, to=3, by=0.1)
y <- sqrt(x)
plot(x, y, type="l")
plot(x, y, type="l", y.lim=c(0,2))
plot(x, y, type="l", ylim=c(0,2))
plot(x, y, type="l", ylim=c(0,2))
x <- seq(from=0, to=3, by=0.01)
y <- sqrt(x)
plot(x, y, type="l", ylim=c(0,2))
max(y)
y=x^2 + 1/2
plot(x, y, type="l", ylim=c(0,2))
quit()
68/30000
30000-68
201/256
95+148
148/243
quit()
data <- matrix(data=c(75, 123, 70, 128), nrow=2, ncol=2, byrow=TRUE)
data
fisher.test(data)
quit()
quit()
(2/3)^2
(1/3)^2
quit()
8
8^8
quit()
1/sqrt(1.5)
1.5^2 + 0.5
0.5^2 + 0.5
quit()
0.8 * 0.09
0.8 - 0.09
0.8 - -0.09
quit()
5/3
quit()
9250 - 5570
5570 + 16582
(5570 + 16582) * 0.03
((5570 + 16582)/1000) * 30
quit()
library(randomForest)
?randomForest
quit()
(1/2) * log((1-(1/8))/(1/8))
quit()
data <- data.frame(
x=c(1, 2, 3)
y=c(1.2, 2.2, 4))
data <- data.frame(
x=c(1, 2, 3),
y=c(1.2, 2.2, 4))
summary(lm(y ~ x, data=data))
summary(lm(x ~ y, data=data))
stuff <- summary(lm(y ~ x, data=data))
stuff$residuals^2
sum(stuff$residuals^2)
stuff <- summary(lm(x ~ y, data=data))
stuff$residuals^2
sum(stuff$residuals^2)
quit()
library(ggplot2)
library(cowplot)
## NOTE: The data used in this demo comes from the UCI machine learning
## repository.
## http://archive.ics.uci.edu/ml/index.php
## Specifically, this is the heart disease data set.
## http://archive.ics.uci.edu/ml/datasets/Heart+Disease
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
data <- read.csv(url, header=FALSE)
#####################################
##
## Reformat the data so that it is
## 1) Easy to use (add nice column names)
## 2) Interpreted correctly by glm()..
##
#####################################
head(data) # you see data, but no column names
colnames(data) <- c(
"age",
"sex",# 0 = female, 1 = male
"cp", # chest pain
# 1 = typical angina,
# 2 = atypical angina,
# 3 = non-anginal pain,
# 4 = asymptomatic
"trestbps", # resting blood pressure (in mm Hg)
"chol", # serum cholestoral in mg/dl
"fbs",  # fasting blood sugar if less than 120 mg/dl, 1 = TRUE, 0 = FALSE
"restecg", # resting electrocardiographic results
# 1 = normal
# 2 = having ST-T wave abnormality
# 3 = showing probable or definite left ventricular hypertrophy
"thalach", # maximum heart rate achieved
"exang",   # exercise induced angina, 1 = yes, 0 = no
"oldpeak", # ST depression induced by exercise relative to rest
"slope", # the slope of the peak exercise ST segment
# 1 = upsloping
# 2 = flat
# 3 = downsloping
"ca", # number of major vessels (0-3) colored by fluoroscopy
"thal", # this is short of thalium heart scan
# 3 = normal (no cold spots)
# 6 = fixed defect (cold spots during rest and exercise)
# 7 = reversible defect (when cold spots only appear during exercise)
"hd" # (the predicted attribute) - diagnosis of heart disease
# 0 if less than or equal to 50% diameter narrowing
# 1 if greater than 50% diameter narrowing
)
head(data) # now we have data and column names
str(data) # this shows that we need to tell R which columns contain factors
# it also shows us that there are some missing values. There are "?"s
# in the dataset. These are in the "ca" and "thal" columns...
## First, convert "?"s to NAs...
data[data == "?"] <- NA
## Now add factors for variables that are factors and clean up the factors
## that had missing data...
data[data$sex == 0,]$sex <- "F"
data[data$sex == 1,]$sex <- "M"
data$sex <- as.factor(data$sex)
data$cp <- as.factor(data$cp)
data$fbs <- as.factor(data$fbs)
data$restecg <- as.factor(data$restecg)
data$exang <- as.factor(data$exang)
data$slope <- as.factor(data$slope)
data$ca <- as.integer(data$ca) # since this column had "?"s in it
# R thinks that the levels for the factor are strings, but
# we know they are integers, so first convert the strings to integiers...
data$ca <- as.factor(data$ca)  # ...then convert the integers to factor levels
data$thal <- as.integer(data$thal) # "thal" also had "?"s in it.
data$thal <- as.factor(data$thal)
## This next line replaces 0 and 1 with "Healthy" and "Unhealthy"
data$hd <- ifelse(test=data$hd == 0, yes="Healthy", no="Unhealthy")
data$hd <- as.factor(data$hd) # Now convert to a factor
str(data) ## this shows that the correct columns are factors
## Now determine how many rows have "NA" (aka "Missing data"). If it's just
## a few, we can remove them from the dataset, otherwise we should consider
## imputing the values with a Random Forest or some other imputation method.
nrow(data[is.na(data$ca) | is.na(data$thal),])
data[is.na(data$ca) | is.na(data$thal),]
## so 6 of the 303 rows of data have missing values. This isn't a large
## percentage (2%), so we can just remove them from the dataset
## NOTE: This is different from when we did machine learning with
## Random Forests. When we did that, we imputed values.
nrow(data)
data <- data[!(is.na(data$ca) | is.na(data$thal)),]
nrow(data)
#####################################
##
## Now we can do some quality control by making sure all of the factor
## levels are represented by people with and without heart disease (hd)
##
## NOTE: We also want to exclude variables that only have 1 or 2 samples in
## a category since +/- one or two samples can have a large effect on the
## odds/log(odds)
##
##
#####################################
xtabs(~ hd + sex, data=data)
xtabs(~ hd + cp, data=data)
xtabs(~ hd + fbs, data=data)
xtabs(~ hd + restecg, data=data)
xtabs(~ hd + exang, data=data)
xtabs(~ hd + slope, data=data)
xtabs(~ hd + ca, data=data)
xtabs(~ hd + thal, data=data)
#####################################
##
## Now we are ready for some logistic regression. First we'll create a very
## simple model that uses sex to predict heart disease
##
#####################################
## let's start super simple and see if sex (female/male) is a good
## predictor...
## First, let's just look at the raw data...
xtabs(~ hd + sex, data=data)
#           sex
# hd         F   M
# Healthy    71  89
# Unhealthy  25 112
## Most of the females are healthy and most of the males are unhealthy.
## Being female is likely to decrease the odds in being unhealthy.
##    In other words, if a sample is female, the odds are against it that it
##    will be unhealthy
## Being male is likely to increase the odds in being unhealthy...
##    In other words, if a sample is male, the odds are for it being unhealthy
###########
##
## Now do the actual logistic regression
##
###########
logistic <- glm(hd ~ sex, data=data, family="binomial")
summary(logistic)
## (Intercept)  -1.0438     0.2326  -4.488 7.18e-06 ***
##   sexM        1.2737     0.2725   4.674 2.95e-06 ***
## Let's start by going through the first coefficient...
## (Intercept)  -1.0438     0.2326  -4.488 7.18e-06 ***
##
## The intercept is the log(odds) a female will be unhealthy. This is because
## female is the first factor in "sex" (the factors are ordered,
## alphabetically by default,"female", "male")
female.log.odds <- log(25 / 71)
female.log.odds
## Now let's look at the second coefficient...
##   sexM        1.2737     0.2725   4.674 2.95e-06 ***
##
## sexM is the log(odds ratio) that tells us that if a sample has sex=M, the
## odds of being unhealthy are, on a log scale, 1.27 times greater than if
## a sample has sex=F.
male.log.odds.ratio <- log((112 / 89) / (25/71))
male.log.odds.ratio
## Now calculate the overall "Pseudo R-squared" and its p-value
## NOTE: Since we are doing logistic regression...
## Null devaince = 2*(0 - LogLikelihood(null model))
##               = -2*LogLikihood(null model)
## Residual deviacne = 2*(0 - LogLikelihood(proposed model))
##                   = -2*LogLikelihood(proposed model)
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
## chi-square value = 2*(LL(Proposed) - LL(Null))
## p-value = 1 - pchisq(chi-square value, df = 2-1)
1 - pchisq(2*(ll.proposed - ll.null), df=1)
1 - pchisq((logistic$null.deviance - logistic$deviance), df=1)
## Lastly, let's  see what this logistic regression predicts, given
## that a patient is either female or male (and no other data about them).
predicted.data <- data.frame(
probability.of.hd=logistic$fitted.values,
sex=data$sex)
## We can plot the data...
ggplot(data=predicted.data, aes(x=sex, y=probability.of.hd)) +
geom_point(aes(color=sex), size=5) +
xlab("Sex") +
ylab("Predicted probability of getting heart disease")
## Since there are only two probabilities (one for females and one for males),
## we can use a table to summarize the predicted probabilities.
xtabs(~ probability.of.hd + sex, data=predicted.data)
#####################################
##
## Now we will use all of the data available to predict heart disease
##
#####################################
logistic <- glm(hd ~ ., data=data, family="binomial")
summary(logistic)
head(data)
test.data <- data[,1:13]
head(test.data)
predict(logistic, newdata=test.data, type="reponse")
predict(logistic, newdata=test.data, type="response")
stuff <- predict(logistic, newdata=test.data, type="response")
logistic$fitted.values
logistic$fitted.values == stuff
table(data$hd, data$sex)
quit()
809504/28
quit()
setwd("~/Google Drive/stat_quests/jupyter_noteboks_python/statquest_xgboost_in_python/")
data <- read.delim(file="Telco_customer_churn.csv", sep=",", stringsAsFactors=FALSE, header=TRUE)
data[,1:2]
data[1:2,]
str(data)
data[1:4, "Total.Charges"]
data[1:4, c("Customer ID", "Total.Charges")]
data[1:4, c("CustomerID", "Total.Charges")]
data$Total.Charges[1:100]
data$Total.Charges[101:200]
data$Total.Charges[201:1000]
data$Total.Charges[1001:2000]
data$Total.Charges[2001:2500]
sum(is.na(data$Total.Charges))
data[is.na(data$Total.Charges),]
quit()
